{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff851b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff218969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0] if tensor else []\n",
    "    return sizes\n",
    "\n",
    "\n",
    "assert shape([1, 2, 3]) == [3]\n",
    "assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]\n",
    "assert shape([\n",
    "    [[1, 2, 3, 4],\n",
    "     [5, 6, 7, 8],\n",
    "     [9, 10, 11, 12]],\n",
    "    [[13, 14, 15, 16],\n",
    "     [17, 18, 19, 20],\n",
    "     [21, 22, 23, 24]]\n",
    "]) == [2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e700e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_1d(tensor: Tensor) -> bool: \n",
    "    \"\"\" \n",
    "    If tensor[0] is a list, it's a higher-order tensor. \n",
    "    Otherwise, tensor is 1-dimensional (that is, a vector). \n",
    "    \"\"\" \n",
    "    return not isinstance(tensor[0], list) \n",
    " \n",
    "assert is_1d([1, 2, 3]) \n",
    "assert not is_1d([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d305374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_sum(tensor: Tensor) -> float: \n",
    "    \"\"\"Sums up all the values in the tensor\"\"\" \n",
    "    if is_1d(tensor): \n",
    "        return sum(tensor)  # just a list of floats, use Python sum \n",
    "    else: \n",
    "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row \n",
    "                   for tensor_i in tensor)   # and sum up those results. \n",
    " \n",
    "assert tensor_sum([1, 2, 3]) == 6 \n",
    "assert tensor_sum([[1, 2], [3, 4]]) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acd70960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Applies a function to each element in the tensor.\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, sub_tensor) for sub_tensor in tensor]\n",
    "\n",
    "assert tensor_apply(lambda x: x + 1, [1, 2, 3]) == [2, 3, 4] \n",
    "assert tensor_apply(lambda x: 2 * x, [[1, 2], [3, 4]]) == [[2, 4], [6, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f0c9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def tensor_combine(f: Callable[[float, float], float], \n",
    "                   tensor1: Tensor, tensor2: Tensor) -> Tensor:\n",
    "    \"\"\"Combines two tensors element-wise using a binary function.\"\"\"\n",
    "    if is_1d(tensor1) and is_1d(tensor2):\n",
    "        return [f(x, y) for x, y in zip(tensor1, tensor2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, sub_tensor1, sub_tensor2)\n",
    "                for sub_tensor1, sub_tensor2 in zip(tensor1, tensor2)]\n",
    "\n",
    "assert tensor_combine(operator.add, [1, 2, 3], [4, 5, 6]) == [5, 7, 9]\n",
    "assert tensor_combine(operator.mul, [1, 2, 3], [4, 5, 6]) == [4, 10, 18]\n",
    "assert tensor_combine(operator.mul, [[1, 2], [3, 4]], [[5, 6], [7, 8]]) == [[5, 12], [21, 32]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "\n",
    "class Layer:\n",
    "    \"\"\" \n",
    "    Our neural networks will be composed of Layers, each of which \n",
    "    knows how to do some computation on its inputs in the \"forward\" \n",
    "    direction and propagate gradients in the \"backward\" direction. \n",
    "    \"\"\"\n",
    "    def forward(self, input): \n",
    "        \"\"\" \n",
    "        Note the lack of types. We're not going to be prescriptive \n",
    "        about what kinds of inputs layers can take and what kinds \n",
    "        of outputs they can return. \n",
    "        \"\"\" \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient): \n",
    "        \"\"\" \n",
    "        Similarly, we're not going to be prescriptive about what the \n",
    "        gradient looks like. It's up to you the user to make sure \n",
    "        that you're doing things sensibly. \n",
    "        \"\"\" \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]: \n",
    "        \"\"\" \n",
    "        Returns the parameters of this layer. The default implementation \n",
    "        returns nothing, so that if you have a layer with no parameters \n",
    "        you don't have to implement this. \n",
    "        \"\"\" \n",
    "        return ()\n",
    "    \n",
    "    def grads(self) -> Iterable[Tensor]: \n",
    "        \"\"\" \n",
    "        Returns the gradients, in the same order as params(). \n",
    "        \"\"\" \n",
    "        return ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.neural_networks import sigmoid\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        self.sigmoids = tensor_apply(sigmoid, input)\n",
    "        return self.sigmoids        \n",
    "    \n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(\n",
    "            lambda sig, grad: sig * (1 - sig) * grad,\n",
    "            self.sigmoids, \n",
    "            gradient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_from_scratch_3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
